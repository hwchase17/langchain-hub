{
    "memory": null,
    "verbose": false,
    "input_key": "input_documents",
    "output_key": "output_text",
    "llm_chain": {
        "memory": null,
        "verbose": false,
        "prompt": {
            "input_variables": [
                "context",
                "question"
            ],
            "output_parser": {
                "regex": "(.*?)\\nScore: (.*)",
                "output_keys": [
                    "answer",
                    "score"
                ],
                "default_output_key": null,
                "_type": "regex_parser"
            },
            "template": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nIn addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n\nQuestion: [question here]\nHelpful Answer: [answer here]\nScore: [score between 0 and 100]\n\nHow to determine the score:\n- Higher is a better answer\n- Better responds fully to the asked question, with sufficient level of detail\n- If you do not know the answer based on the context, that should be a score of 0\n- Don't be overconfident!\n\nExample #1\n\nContext:\n---------\nApples are red\n---------\nQuestion: what color are apples?\nHelpful Answer: red\nScore: 100\n\nExample #2\n\nContext:\n---------\nit was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n---------\nQuestion: what type was the car?\nHelpful Answer: a sports car or an suv\nScore: 60\n\nExample #3\n\nContext:\n---------\nPears are either red or orange\n---------\nQuestion: what color are apples?\nHelpful Answer: This document does not answer the question\nScore: 0\n\nBegin!\n\nContext:\n---------\n{context}\n---------\nQuestion: {question}\nHelpful Answer:",
            "template_format": "f-string",
            "_type": "prompt"
        },
        "llm": {
            "model_name": "text-davinci-003",
            "temperature": 0.0,
            "max_tokens": 256,
            "top_p": 1,
            "frequency_penalty": 0,
            "presence_penalty": 0,
            "n": 1,
            "best_of": 1,
            "request_timeout": null,
            "logit_bias": {},
            "_type": "openai"
        },
        "output_key": "text",
        "_type": "llm_chain"
    },
    "document_variable_name": "context",
    "rank_key": "score",
    "answer_key": "answer",
    "metadata_keys": null,
    "return_intermediate_steps": true,
    "_type": "map_rerank_documents_chain"
}