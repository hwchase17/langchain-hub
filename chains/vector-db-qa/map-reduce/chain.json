{
    "memory": null,
    "verbose": false,
    "k": 4,
    "combine_documents_chain": {
        "memory": null,
        "verbose": false,
        "input_key": "input_documents",
        "output_key": "output_text",
        "llm_chain": {
            "memory": null,
            "verbose": false,
            "prompt": {
                "input_variables": [
                    "context",
                    "question"
                ],
                "output_parser": null,
                "template": "Use the following portion of a long document to see if any of the text is relevant to answer the question. \nReturn any relevant text verbatim.\n{context}\nQuestion: {question}\nRelevant text, if any:",
                "template_format": "f-string",
                "_type": "prompt"
            },
            "llm": {
                "model_name": "text-davinci-003",
                "temperature": 0.7,
                "max_tokens": 256,
                "top_p": 1,
                "frequency_penalty": 0,
                "presence_penalty": 0,
                "n": 1,
                "best_of": 1,
                "request_timeout": null,
                "logit_bias": {},
                "_type": "openai"
            },
            "output_key": "text",
            "_type": "llm_chain"
        },
        "combine_document_chain": {
            "memory": null,
            "verbose": false,
            "input_key": "input_documents",
            "output_key": "output_text",
            "llm_chain": {
                "memory": null,
                "verbose": false,
                "prompt": {
                    "input_variables": [
                        "summaries",
                        "question"
                    ],
                    "output_parser": null,
                    "template": "Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\").\nIf you don't know the answer, just say that you don't know. Don't try to make up an answer.\nALWAYS return a \"SOURCES\" part in your answer.\n\nQUESTION: {question}\n=========\n{summaries}\n=========\nFINAL_ANSWER:",
                    "template_format": "f-string",
                    "_type": "prompt"
                },
                "llm": {
                    "model_name": "text-davinci-003",
                    "temperature": 0.7,
                    "max_tokens": 256,
                    "top_p": 1,
                    "frequency_penalty": 0,
                    "presence_penalty": 0,
                    "n": 1,
                    "best_of": 1,
                    "request_timeout": null,
                    "logit_bias": {},
                    "_type": "openai"
                },
                "output_key": "text",
                "_type": "llm_chain"
            },
            "document_prompt": {
                "input_variables": [
                    "page_content"
                ],
                "output_parser": null,
                "template": "{page_content}",
                "template_format": "f-string",
                "_type": "prompt"
            },
            "document_variable_name": "summaries",
            "_type": "stuff_documents_chain"
        },
        "collapse_document_chain": null,
        "document_variable_name": "context",
        "return_intermediate_steps": false,
        "_type": "map_reduce_documents_chain"
    },
    "input_key": "query",
    "output_key": "result",
    "return_source_documents": false,
    "search_kwargs": {},
    "_type": "vector_db_qa"
}